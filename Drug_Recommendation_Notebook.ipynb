{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOvouH9NihN/KxwJel+yCXF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NoureldinAyman/Drug-Recommendation/blob/main/Drug_Recommendation_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Drug Recommendation System using MIMIC-IV"
      ],
      "metadata": {
        "id": "wW3Q1MzCpPX6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project implements a sophisticated machine learning pipeline to create a **drug recommendation system**, designed to function as a clinical decision support tool. The primary objective is to accurately predict the specific medications a patient is likely to be prescribed during a hospital admission, based on a comprehensive view of their clinical profile.\n",
        "\n",
        "### Methodology and Data\n",
        "\n",
        "The project leverages the **MIMIC-IV (Medical Information Mart for Intensive Care)** dataset, a large, de-identified database containing detailed patient information from critical care units.\n",
        "\n",
        "A key innovation of this project is its prediction target. Instead of merely predicting a drug's name, the model predicts a **composite drug label**. This granular label combines three critical pieces of information:\n",
        "* **NDC (National Drug Code):** The specific drug product.\n",
        "* **Dosage Strength:** The concentration of the medication.\n",
        "* **Prescription Duration:** The length of the treatment course.\n",
        "\n",
        "This approach provides a much more clinically actionable prediction compared to generic drug recommendations.\n",
        "\n",
        "#### Feature Engineering and Modeling\n",
        "\n",
        "The model's predictive power is built on a rich, multi-modal feature set that fuses both structured and unstructured data.\n",
        "\n",
        "* **Structured Clinical Data:** This includes patient demographics (age, gender), admission details (type, location), insurance status, and codified clinical events like diagnoses and procedures (ICD codes), and emergency department triage data.\n",
        "* **Unstructured Clinical Notes:** To capture the nuanced narrative of a patient's condition, the system performs advanced Natural Language Processing (NLP) on discharge summaries. It utilizes **Bio_ClinicalBERT**, a state-of-the-art transformer model pre-trained specifically on biomedical and clinical text, to generate powerful, context-aware embeddings.\n",
        "\n",
        "These features are then fed into a **deep neural network** architected for multi-label classification, allowing it to predict a unique set of multiple potential medications for each patient.\n",
        "\n",
        "To ensure the model is robust and generalizable, the data is split on a **patient-level basis**. This prevents data leakage, which occurs when information about the same patient appears in both the training and testing sets. This method results in a more realistic and reliable evaluation of the model's performance on truly unseen patients."
      ],
      "metadata": {
        "id": "AuC89AxppV2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Table of Contents\n",
        "\n",
        "- [Setup and Imports](#Setup-and-Imports)\n",
        "- [Data Loading](#Data-Loading)\n",
        "- [Data Cleaning and Type Conversion](#Data-Cleaning-and-Type-Conversion)\n",
        "- [Base Dataframe Creation](#Base-Dataframe-Creation)\n",
        "- [Refined Target Engineering (Composite Drug Labels)](#Target-Engineering-Composite-Drug-Labels)\n",
        "- [Feature Engineering](#Feature-Engineering)\n",
        "    - [Feature Engineering - Demographics and Admissions Data](#FE---Demographics-and-Admissions-Data)\n",
        "    - [Feature Engineering - Diagnoses (ICD Codes)](#FE---Diagnoses-ICD-Codes)\n",
        "    - [Feature Engineering - Procedures (ICD Codes)](#FE---Procedures-ICD-Codes)\n",
        "    - [Feature Engineering - Emergency Department (ED) Data](#FE---Emergency-Department-ED-Data)\n",
        "- [Text Preprocessing and Transformer Embeddings](#Text-Preprocessing-and-Transformer-Embeddings)\n",
        "- [Final Data Assembly](#Final-Data)\n",
        "- [Data Loading for Modeling](#Data-Loading-for-Modeling)\n",
        "- [Train/Validation/Test Split](#TrainValidationTest-Split)\n",
        "- [Model Development](#Model-Development)\n",
        "    - [Model Definition](#Model-Definition)\n",
        "    - [Model Compilation and Training](#Model-Compilation-and-Training)\n",
        "    - [Model Evaluation](#Model-Evaluation)"
      ],
      "metadata": {
        "id": "PRReqXIxpuLr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup and Imports"
      ],
      "metadata": {
        "id": "Pvuu9vMHjk5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import gc\n",
        "import re"
      ],
      "metadata": {
        "id": "tFoPNwnAjlQC"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import ML libraries\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "x8Z5d8vdjxOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount Google Drive to access files stored there\n"
      ],
      "metadata": {
        "id": "UANZLDp6kV0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "m-2-J4u9kVIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the base path for the project directory in Google Drive\n"
      ],
      "metadata": {
        "id": "ce3eLWiSkdJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "project_base_path = \"/content/drive/MyDrive/AIS302 Project/\""
      ],
      "metadata": {
        "id": "eAAXBtDOkeIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading"
      ],
      "metadata": {
        "id": "Xa8Kq_oDkhj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure paths for all data files and specify if they should be read in chunks\n",
        "files_config = {\n",
        "    \"patients\": {\"path\": \"Data/hosp/patients.csv.gz\", \"chunk\": False},\n",
        "    \"admissions\": {\"path\": \"Data/hosp/admissions.csv.gz\", \"chunk\": False},\n",
        "    \"diagnoses_icd\": {\"path\": \"Data/hosp/diagnoses_icd.csv.gz\", \"chunk\": False},\n",
        "    \"d_icd_diagnoses\": {\"path\": \"Data/hosp/d_icd_diagnoses.csv.gz\", \"chunk\": False},\n",
        "    \"prescriptions\": {\"path\": \"Data/hosp/prescriptions.csv.gz\", \"chunk\": True, \"chunksize\": 500000},\n",
        "    \"labevents\": {\"path\": \"Data/hosp/labevents.csv.gz\", \"chunk\": True, \"chunksize\": 500000},\n",
        "    \"d_labitems\": {\"path\": \"Data/hosp/d_labitems.csv.gz\", \"chunk\": False},\n",
        "    \"procedures_icd\": {\"path\": \"Data/hosp/procedures_icd.csv.gz\", \"chunk\": False},\n",
        "    \"emar\": {\"path\": \"Data/hosp/emar.csv.gz\", \"chunk\": True, \"chunksize\": 500000},\n",
        "    \"discharge_notes\": {\"path\": \"Data/note/discharge.csv.gz\", \"chunk\": False},\n",
        "    \"medrecon\": {\"path\": \"Data/ed/medrecon.csv.gz\", \"chunk\": False},\n",
        "    \"triage_ed\": {\"path\": \"Data/ed/triage.csv.gz\", \"chunk\": False},\n",
        "    \"edstays\": {\"path\": \"Data/ed/edstays.csv.gz\", \"chunk\": False}\n",
        "}\n",
        "\n",
        "# Dictionary to hold the loaded dataframes\n",
        "dfs = {}\n",
        "\n",
        "# Loop through the file configuration to load each csv file\n",
        "for name, config in files_config.items():\n",
        "    full_path = os.path.join(project_base_path, config[\"path\"])\n",
        "\n",
        "    # If the file is large, read it in chunks to manage memory usage\n",
        "    if config[\"chunk\"]:\n",
        "        chunk_list = []\n",
        "        reader = pd.read_csv(full_path, compression='gzip', low_memory=False, chunksize=config[\"chunksize\"])\n",
        "\n",
        "        # Iterate over chunks and append them to a list\n",
        "        for i, chunk_df in enumerate(reader):\n",
        "            chunk_list.append(chunk_df)\n",
        "\n",
        "        # Concatenate all chunks into a single dataframe\n",
        "        dfs[name] = pd.concat(chunk_list, ignore_index=True)\n",
        "        del chunk_list # Free up memory\n",
        "        gc.collect()\n",
        "    else:\n",
        "        # Read smaller files directly into a dataframe\n",
        "        dfs[name] = pd.read_csv(full_path, compression='gzip', low_memory=False)\n",
        "    print(f\"  Successfully loaded {name}. Shape: {dfs[name].shape}\")"
      ],
      "metadata": {
        "id": "ckcJn3L_kiGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning and Type Conversion"
      ],
      "metadata": {
        "id": "v1pjRt9KkrX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert date of death column in patients table to datetime objects\n",
        "dfs['patients']['dod'] = pd.to_datetime(dfs['patients']['dod'], errors='coerce')\n",
        "\n",
        "# Convert time-related columns in the admissions table to datetime objects\n",
        "adm_time_cols = ['admittime', 'dischtime', 'deathtime', 'edregtime', 'edouttime']\n",
        "for col in adm_time_cols:\n",
        "    dfs['admissions'][col] = pd.to_datetime(dfs['admissions'][col], errors='coerce')\n",
        "\n",
        "# Convert time-related columns in the prescriptions table to datetime objects\n",
        "presc_time_cols = ['starttime', 'stoptime']\n",
        "for col in presc_time_cols:\n",
        "    dfs['prescriptions'][col] = pd.to_datetime(dfs['prescriptions'][col], errors='coerce')\n",
        "\n",
        "# Convert time-related columns in the labevents table to datetime objects\n",
        "lab_time_cols = ['charttime', 'storetime']\n",
        "for col in lab_time_cols:\n",
        "    dfs['labevents'][col] = pd.to_datetime(dfs['labevents'][col], errors='coerce')\n",
        "\n",
        "# Convert chart date in procedures table to datetime objects\n",
        "dfs['procedures_icd']['chartdate'] = pd.to_datetime(dfs['procedures_icd']['chartdate'], errors='coerce')\n",
        "\n",
        "# Convert time-related columns in the emar (medication administration) table to datetime objects\n",
        "emar_time_cols = ['charttime', 'scheduletime', 'storetime']\n",
        "for col in emar_time_cols:\n",
        "    dfs['emar'][col] = pd.to_datetime(dfs['emar'][col], errors='coerce')\n",
        "\n",
        "# Convert time-related columns in discharge notes to datetime objects\n",
        "discharge_time_cols = ['charttime', 'storetime']\n",
        "for col in discharge_time_cols:\n",
        "    dfs['discharge_notes'][col] = pd.to_datetime(dfs['discharge_notes'][col], errors='coerce')\n",
        "\n",
        "# Convert chart time in medrecon (medication reconciliation) table to datetime objects\n",
        "dfs['medrecon']['charttime'] = pd.to_datetime(dfs['medrecon']['charttime'], errors='coerce')\n",
        "\n",
        "# Convert time-related columns in edstays (emergency department stays) to datetime objects\n",
        "edstays_time_cols = ['intime', 'outtime']\n",
        "for col in edstays_time_cols:\n",
        "    dfs['edstays'][col] = pd.to_datetime(dfs['edstays'][col], errors='coerce')"
      ],
      "metadata": {
        "id": "zzZgAVXIkqw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Base Dataframe Creation"
      ],
      "metadata": {
        "id": "zgawrqH7q-A8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a base dataframe with admission-level information\n",
        "base_df = dfs['admissions'][['subject_id', 'hadm_id', 'admittime', 'dischtime', 'admission_type', 'admission_location', 'discharge_location', 'insurance', 'language', 'marital_status', 'race', 'hospital_expire_flag']].copy()\n",
        "\n",
        "# Merge with patient demographic data (gender, age, date of death)\n",
        "base_df = pd.merge(base_df, dfs['patients'][['subject_id', 'gender', 'anchor_age', 'dod']], on='subject_id', how='left')\n",
        "\n",
        "# Rename the anchor_age column to 'age' for clarity\n",
        "base_df.rename(columns={'anchor_age': 'age'}, inplace=True)\n",
        "\n",
        "# Calculate the length of stay in days for each admission\n",
        "base_df['los_days'] = (base_df['dischtime'] - base_df['admittime']).dt.total_seconds() / (24 * 60 * 60)"
      ],
      "metadata": {
        "id": "wySWXto6rFNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Target Engineering (Composite Drug Labels)"
      ],
      "metadata": {
        "id": "cEl__FBAk1sC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start of refined target variable engineering\n",
        "prescriptions_temp = dfs['prescriptions'].copy()\n",
        "\n",
        "# Filter for relevant admissions and convert time columns\n",
        "prescriptions_temp = prescriptions_temp[prescriptions_temp['hadm_id'].isin(base_df['hadm_id'].unique())]\n",
        "prescriptions_temp['starttime'] = pd.to_datetime(prescriptions_temp['starttime'], errors='coerce')\n",
        "prescriptions_temp['stoptime'] = pd.to_datetime(prescriptions_temp['stoptime'], errors='coerce')\n",
        "\n",
        "# Clean ndc codes by handling nans and removing trailing '.0'\n",
        "prescriptions_temp['ndc'] = prescriptions_temp['ndc'].fillna('__ORIGINAL_NDC_NAN__')\n",
        "prescriptions_temp['ndc'] = prescriptions_temp['ndc'].astype(str)\n",
        "prescriptions_temp['ndc'] = prescriptions_temp['ndc'].str.replace(r'\\.0$', '', regex=True)\n",
        "\n",
        "# Remove records with invalid ndc placeholders\n",
        "invalid_ndc_placeholders = ['0', '__ORIGINAL_NDC_NAN__', 'nan', 'NaN', 'MISSING_NDC', 'UNKNOWN_NDC']\n",
        "prescriptions_temp = prescriptions_temp[~prescriptions_temp['ndc'].isin(invalid_ndc_placeholders)]\n",
        "prescriptions_temp.dropna(subset=['starttime', 'ndc'], inplace=True)\n",
        "\n",
        "# Calculate prescription duration in days\n",
        "duration_hours = (prescriptions_temp['stoptime'] - prescriptions_temp['starttime']).dt.total_seconds() / 3600\n",
        "prescriptions_temp['duration_days'] = duration_hours / 24\n",
        "prescriptions_temp.loc[prescriptions_temp['duration_days'] < 0, 'duration_days'] = 0 # Handle negative durations\n",
        "\n",
        "# Categorize the prescription duration into bins\n",
        "duration_bins = [-float('inf'), 1, 3, 7, 14, 30, float('inf')]\n",
        "duration_labels = ['<=1d', '1-3d', '3-7d', '7-14d', '14-30d', '>30d']\n",
        "prescriptions_temp['duration_category'] = pd.cut(prescriptions_temp['duration_days'], bins=duration_bins, labels=duration_labels, right=True)\n",
        "prescriptions_temp['duration_category'] = prescriptions_temp['duration_category'].cat.add_categories('unknown_duration').fillna('unknown_duration')\n",
        "prescriptions_temp['duration_category'] = prescriptions_temp['duration_category'].astype(str)\n",
        "\n",
        "# Clean the product strength information\n",
        "prescriptions_temp['prod_strength'] = prescriptions_temp['prod_strength'].astype(str)\n",
        "prescriptions_temp['dosage_form_strength'] = prescriptions_temp['prod_strength'].str.lower().str.strip()\n",
        "placeholders_for_strength = ['nan', '', 'none', 'unknown_strength']\n",
        "prescriptions_temp.loc[prescriptions_temp['dosage_form_strength'].isin(placeholders_for_strength) | prescriptions_temp['dosage_form_strength'].isnull(), 'dosage_form_strength'] = 'unknown_strength_cleaned'\n",
        "\n",
        "# Copy the processed prescriptions dataframe and free up memory\n",
        "prescriptions_filtered = prescriptions_temp.copy()\n",
        "del prescriptions_temp\n",
        "\n",
        "# Ensure data types are correct for creating the composite key\n",
        "prescriptions_filtered['ndc'] = prescriptions_filtered['ndc'].astype(str)\n",
        "prescriptions_filtered['dosage_form_strength'] = prescriptions_filtered['dosage_form_strength'].astype(str)\n",
        "prescriptions_filtered['duration_category'] = prescriptions_filtered['duration_category'].astype(str)\n",
        "\n",
        "# Filter for prescriptions with valid ndc, strength, and duration\n",
        "valid_ndc_prescriptions = prescriptions_filtered[ prescriptions_filtered['ndc'].str.match(r'^[0-9\\-]+$') & (prescriptions_filtered['ndc'].str.len() >= 4)].copy()\n",
        "valid_ndc_prescriptions = valid_ndc_prescriptions[valid_ndc_prescriptions['dosage_form_strength'] != 'unknown_strength_cleaned']\n",
        "valid_ndc_prescriptions = valid_ndc_prescriptions[valid_ndc_prescriptions['duration_category'] != 'unknown_duration']\n",
        "\n",
        "# Create a composite target label by combining ndc, dosage strength, and duration category\n",
        "valid_ndc_prescriptions['composite_target_label'] = \\\n",
        "    valid_ndc_prescriptions['ndc'] + \"_\" + \\\n",
        "    valid_ndc_prescriptions['dosage_form_strength'] + \"_\" + \\\n",
        "    valid_ndc_prescriptions['duration_category']\n",
        "\n",
        "# Determine the top N composite targets to use for prediction\n",
        "TOP_N_COMPOSITE_TARGETS = 200\n",
        "if len(valid_ndc_prescriptions['composite_target_label'].unique()) < TOP_N_COMPOSITE_TARGETS:\n",
        "    TOP_N_COMPOSITE_TARGETS = len(valid_ndc_prescriptions['composite_target_label'].unique())\n",
        "composite_target_counts = valid_ndc_prescriptions['composite_target_label'].value_counts()\n",
        "if TOP_N_COMPOSITE_TARGETS == 0 :\n",
        "    final_target_labels_to_predict = []\n",
        "else:\n",
        "    final_target_labels_to_predict = composite_target_counts.head(TOP_N_COMPOSITE_TARGETS).index.tolist()\n",
        "\n",
        "# Filter for prescriptions that fall into our final target labels\n",
        "relevant_prescriptions_for_y = valid_ndc_prescriptions[\n",
        "    valid_ndc_prescriptions['composite_target_label'].isin(final_target_labels_to_predict) &\n",
        "    valid_ndc_prescriptions['hadm_id'].isin(base_df['hadm_id'].unique())\n",
        "]\n",
        "# Group by admission to get all composite labels for each\n",
        "hadm_composite_drugs = relevant_prescriptions_for_y.groupby('hadm_id')['composite_target_label'].apply(lambda x: list(set(x))).reset_index()\n",
        "\n",
        "# Build the new multi-label target dataframe based on the composite labels\n",
        "new_target_y_list = []\n",
        "for hadm_id_val in base_df['hadm_id'].unique():\n",
        "    labels_for_hadm = hadm_composite_drugs[hadm_composite_drugs['hadm_id'] == hadm_id_val]\n",
        "    current_hadm_label_vector = {label: 0 for label in final_target_labels_to_predict}\n",
        "    if not labels_for_hadm.empty:\n",
        "        prescribed_labels_for_hadm = labels_for_hadm.iloc[0]['composite_target_label']\n",
        "        for label in prescribed_labels_for_hadm:\n",
        "            if label in current_hadm_label_vector:\n",
        "                current_hadm_label_vector[label] = 1\n",
        "    current_hadm_label_vector['hadm_id'] = hadm_id_val\n",
        "    new_target_y_list.append(current_hadm_label_vector)\n",
        "new_target_y_df = pd.DataFrame(new_target_y_list)\n",
        "new_target_y_df = new_target_y_df.set_index('hadm_id')\n",
        "\n",
        "# Ensure the base dataframe is indexed by hadm_id for merging\n",
        "if base_df.index.name != 'hadm_id':\n",
        "    base_df_indexed = base_df.set_index('hadm_id')\n",
        "else:\n",
        "    base_df_indexed = base_df.copy()\n",
        "\n",
        "# Merge the new composite target labels into the analytical dataframe\n",
        "analytical_df = pd.merge(base_df_indexed, new_target_y_df, left_index=True, right_index=True, how='left')\n",
        "# Fill any missing values in the new target columns with 0\n",
        "analytical_df[final_target_labels_to_predict] = analytical_df[final_target_labels_to_predict].fillna(0).astype(int)"
      ],
      "metadata": {
        "id": "YKYn3b9mkz0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "5w1kQOQ9qqcX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FE - Demographics and Admissions Data"
      ],
      "metadata": {
        "id": "-f071pYflHLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess categorical features: fill missing values with 'Unknown'\n",
        "cols_to_fill_na_unknown = ['admission_location', 'discharge_location', 'insurance', 'language', 'marital_status']\n",
        "for col in cols_to_fill_na_unknown:\n",
        "    if col in analytical_df.columns:\n",
        "        analytical_df[col] = analytical_df[col].fillna('Unknown')\n",
        "\n",
        "# Define categorical columns to be one-hot encoded\n",
        "categorical_cols_to_one_hot = ['admission_type', 'admission_location', 'discharge_location', 'insurance', 'language', 'marital_status', 'race', 'gender']\n",
        "# Apply one-hot encoding to convert categorical variables into a numerical format\n",
        "analytical_df = pd.get_dummies(analytical_df, columns=categorical_cols_to_one_hot, prefix=categorical_cols_to_one_hot, dummy_na=False)\n",
        "\n",
        "# Drop columns that are no longer needed\n",
        "analytical_df = analytical_df.drop(columns=['subject_id', 'dod'])"
      ],
      "metadata": {
        "id": "kcuYhB5mlMO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FE - Diagnoses (ICD Codes)"
      ],
      "metadata": {
        "id": "ZAlRYU7-lPnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add diagnosis information (ICD codes)\n",
        "diagnoses = dfs['diagnoses_icd'][['hadm_id', 'icd_code']].copy()\n",
        "\n",
        "# Identify the top 100 most common diagnosis codes\n",
        "TOP_N_ICD_CODES = 100\n",
        "common_icd_codes = diagnoses['icd_code'].value_counts().head(TOP_N_ICD_CODES).index.tolist()\n",
        "\n",
        "# Filter for diagnoses relevant to our admissions and common codes\n",
        "diag_filtered_for_hot_encode = diagnoses[diagnoses['icd_code'].isin(common_icd_codes) & diagnoses['hadm_id'].isin(analytical_df.index)]\n",
        "\n",
        "# Group by admission to get a list of diagnoses for each\n",
        "hadm_icd_codes = diag_filtered_for_hot_encode.groupby('hadm_id')['icd_code'].apply(list).reset_index()\n",
        "\n",
        "# Create binary features for each of the top 100 diagnoses\n",
        "diag_feature_list = []\n",
        "for hadm_id_val in analytical_df.index:\n",
        "    codes_for_hadm = hadm_icd_codes[hadm_icd_codes['hadm_id'] == hadm_id_val]\n",
        "    current_hadm_icd_vector = {f\"diag_{code}\": 0 for code in common_icd_codes}\n",
        "    if not codes_for_hadm.empty:\n",
        "        icd_list_for_hadm = codes_for_hadm.iloc[0]['icd_code']\n",
        "        for code in icd_list_for_hadm:\n",
        "            if f\"diag_{code}\" in current_hadm_icd_vector:\n",
        "                current_hadm_icd_vector[f\"diag_{code}\"] = 1\n",
        "    current_hadm_icd_vector['hadm_id'] = hadm_id_val\n",
        "    diag_feature_list.append(current_hadm_icd_vector)\n",
        "\n",
        "diag_features_df = pd.DataFrame(diag_feature_list)\n",
        "diag_features_df = diag_features_df.set_index('hadm_id')\n",
        "# Merge diagnosis features into the analytical dataframe\n",
        "analytical_df = analytical_df.merge(diag_features_df, on='hadm_id', how='left')\n",
        "for col in diag_features_df.columns:\n",
        "    analytical_df[col] = analytical_df[col].fillna(0).astype(int)"
      ],
      "metadata": {
        "id": "3nU4EYEHlPKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FE - Procedures (ICD Codes)"
      ],
      "metadata": {
        "id": "uh7afNTBlPUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add procedure information (ICD codes)\n",
        "procedures_df = dfs['procedures_icd'][['hadm_id', 'icd_code', 'icd_version']].copy()\n",
        "\n",
        "# Identify the top 50 most common procedure codes\n",
        "TOP_N_PROC_CODES = 50\n",
        "common_proc_codes = procedures_df['icd_code'].value_counts().head(TOP_N_PROC_CODES).index.tolist()\n",
        "\n",
        "# Filter for procedures relevant to our admissions and common codes\n",
        "proc_filtered_for_hot_encode = procedures_df[procedures_df['icd_code'].isin(common_proc_codes) & procedures_df['hadm_id'].isin(analytical_df.index)]\n",
        "\n",
        "# Group by admission to get a list of procedures for each\n",
        "hadm_proc_codes = proc_filtered_for_hot_encode.groupby('hadm_id')['icd_code'].apply(list).reset_index()\n",
        "\n",
        "# Create binary features for each of the top 50 procedures\n",
        "proc_feature_list = []\n",
        "for hadm_id_val in analytical_df.index:\n",
        "    codes_for_hadm = hadm_proc_codes[hadm_proc_codes['hadm_id'] == hadm_id_val]\n",
        "    current_hadm_proc_vector = {f\"proc_{str(code).replace('.', '_')}\": 0 for code in common_proc_codes}\n",
        "\n",
        "    if not codes_for_hadm.empty:\n",
        "        proc_list_for_hadm = codes_for_hadm.iloc[0]['icd_code']\n",
        "        for code in proc_list_for_hadm:\n",
        "            clean_code_col = f\"proc_{str(code).replace('.', '_')}\"\n",
        "\n",
        "            if clean_code_col in current_hadm_proc_vector:\n",
        "                current_hadm_proc_vector[clean_code_col] = 1\n",
        "    current_hadm_proc_vector['hadm_id'] = hadm_id_val\n",
        "    proc_feature_list.append(current_hadm_proc_vector)\n",
        "\n",
        "proc_features_df = pd.DataFrame(proc_feature_list)\n",
        "proc_features_df = proc_features_df.set_index('hadm_id')\n",
        "\n",
        "# Merge procedure features into the analytical dataframe\n",
        "analytical_df = analytical_df.merge(proc_features_df, on='hadm_id', how='left')\n",
        "for col in proc_features_df.columns:\n",
        "    if col in analytical_df.columns:\n",
        "        analytical_df[col] = analytical_df[col].fillna(0).astype(int)"
      ],
      "metadata": {
        "id": "SzZhJUxylgLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FE - Emergency Department (ED) Data"
      ],
      "metadata": {
        "id": "yRrQtYJalsKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add emergency department (ED) data\n",
        "edstays_df = dfs['edstays'][['hadm_id', 'stay_id', 'intime', 'outtime']].copy()\n",
        "edstays_df.dropna(subset=['hadm_id'], inplace=True)\n",
        "edstays_df['hadm_id'] = edstays_df['hadm_id'].astype(analytical_df.index.dtype)\n",
        "\n",
        "# Calculate length of stay in the ED in hours\n",
        "edstays_df['intime'] = pd.to_datetime(edstays_df['intime'], errors='coerce')\n",
        "edstays_df['outtime'] = pd.to_datetime(edstays_df['outtime'], errors='coerce')\n",
        "edstays_df['ed_los_hours'] = (edstays_df['outtime'] - edstays_df['intime']).dt.total_seconds() / 3600\n",
        "edstays_df.loc[edstays_df['ed_los_hours'] < 0, 'ed_los_hours'] = np.nan\n",
        "\n",
        "# Handle cases where a single hospital admission is linked to multiple ED stays by keeping the latest one\n",
        "edstays_df = edstays_df.sort_values(by=['hadm_id', 'outtime'], ascending=[True, False])\n",
        "edstays_df = edstays_df.drop_duplicates(subset=['hadm_id'], keep='first')\n",
        "edstays_processed = edstays_df[['hadm_id', 'stay_id', 'ed_los_hours']].set_index('hadm_id')\n",
        "\n",
        "# Process triage data from the ED\n",
        "triage_df = dfs['triage_ed'].copy()\n",
        "vital_cols = ['temperature', 'heartrate', 'resprate', 'o2sat', 'sbp', 'dbp', 'pain']\n",
        "\n",
        "# Clean and impute missing vital signs with the median value\n",
        "for col in vital_cols:\n",
        "    triage_df[col] = pd.to_numeric(triage_df[col], errors='coerce')\n",
        "    median_val = triage_df[col].median()\n",
        "    triage_df[col] = triage_df[col].fillna(median_val)\n",
        "    triage_df.rename(columns={col: f\"ed_{col}_triage\"}, inplace=True)\n",
        "\n",
        "# Process patient acuity score from triage\n",
        "triage_df['ed_acuity'] = pd.to_numeric(triage_df['acuity'], errors='coerce')\n",
        "triage_df['ed_acuity'] = triage_df['ed_acuity'].fillna(triage_df['ed_acuity'].mode()[0] if not triage_df['ed_acuity'].mode(dropna=True).empty else 0).astype(int)\n",
        "\n",
        "# Select the processed triage features\n",
        "processed_vital_cols_renamed = [f\"ed_{col}_triage\" for col in vital_cols if f\"ed_{col}_triage\" in triage_df.columns]\n",
        "acuity_col_name = ['ed_acuity'] if 'ed_acuity' in triage_df.columns else []\n",
        "triage_features_cols_to_select = ['stay_id'] + processed_vital_cols_renamed + acuity_col_name\n",
        "triage_features_cols_to_select = [col for col in triage_features_cols_to_select if col in triage_df.columns or col == 'stay_id']\n",
        "triage_features = triage_df[triage_features_cols_to_select].copy()\n",
        "\n",
        "# Process medication reconciliation data from the ED\n",
        "medrecon_df = dfs['medrecon'].copy()\n",
        "\n",
        "# Count the number of medications recorded for each ED stay\n",
        "medrecon_counts = medrecon_df.groupby('stay_id').size().reset_index(name='ed_medrecon_count')\n",
        "\n",
        "# Combine all ED features (length of stay, triage, medrecon)\n",
        "ed_features_combined = edstays_processed.reset_index().merge(triage_features, on='stay_id', how='left')\n",
        "ed_features_combined = ed_features_combined.merge(medrecon_counts, on='stay_id', how='left')\n",
        "ed_features_combined['ed_medrecon_count'].fillna(0, inplace=True)\n",
        "\n",
        "# Clean up and set index for merging\n",
        "ed_features_combined = ed_features_combined.drop(columns=['stay_id'])\n",
        "ed_features_final = ed_features_combined.set_index('hadm_id')\n",
        "\n",
        "# One-hot encode the ED acuity score\n",
        "ed_features_final = pd.get_dummies(ed_features_final, columns=['ed_acuity'], prefix='ed_acuity', dummy_na=False)\n",
        "\n",
        "# Merge the final ED features into the main analytical dataframe\n",
        "analytical_df = analytical_df.merge(ed_features_final, on='hadm_id', how='left')\n",
        "\n",
        "# Fill missing values for the newly added ED features\n",
        "new_ed_cols = ed_features_final.columns.tolist()\n",
        "for col in new_ed_cols:\n",
        "    if analytical_df[col].dtype == 'bool' or col.startswith('ed_acuity_'):\n",
        "        analytical_df[col] = analytical_df[col].fillna(False).astype(bool)\n",
        "    else:\n",
        "        analytical_df[col] = analytical_df[col].fillna(0)"
      ],
      "metadata": {
        "id": "-f3YI5wdlsu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Preprocessing and Transformer Embeddings"
      ],
      "metadata": {
        "id": "9ZWILz1WlzGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fefine a function to clean clinical notes\n",
        "def clean_clinical_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove de-identification placeholders like [** ... **]\n",
        "    text = re.sub(r'\\[\\*\\*.*?\\*\\*\\]', ' ', text)\n",
        "    text = text.replace('___', ' ')\n",
        "\n",
        "    # Remove boilerplate headers like patient name, admission date, etc.\n",
        "    boilerplate_patterns = [\n",
        "        r\"^\\s*name\\s*:.*?\\n\", r\"^\\s*unit no\\s*:.*?\\n\", r\"^\\s*admission date\\s*:.*?\\n\",\n",
        "        r\"^\\s*discharge date\\s*:.*?\\n\", r\"^\\s*date of birth\\s*:.*?\\n\", r\"^\\s*sex\\s*:.*?\\n\",\n",
        "        r\"^\\s*service\\s*:.*?\\n\", r\"^\\s*allergies\\s*:.*?\\n\", r\"^\\s*attending\\s*:.*?\\n\"\n",
        "    ]\n",
        "    for pattern in boilerplate_patterns:\n",
        "        text = re.sub(pattern, '', text, flags=re.IGNORECASE | re.MULTILINE)\n",
        "\n",
        "    # Remove section headers by replacing colon with a space\n",
        "    text = re.sub(r'([a-z\\s]+):\\s*', r'\\1 ', text)\n",
        "\n",
        "    # Normalize whitespace and newlines\n",
        "    text = text.replace('\\n', ' ')\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.strip()\n",
        "\n",
        "    # Remove special characters\n",
        "    text = text.replace('*', '')\n",
        "    text = text.replace('#', '')\n",
        "    return text\n",
        "\n",
        "# Set up paths for processing clinical notes\n",
        "cleaned_notes_parquet_path = os.path.join(project_base_path, \"cleaned_discharge_notes.parquet\")\n",
        "notes_for_processing = None\n",
        "\n",
        "# Check if cleaned notes already exist to save processing time\n",
        "if os.path.exists(cleaned_notes_parquet_path):\n",
        "    print(\"Loading pre-cleaned notes from parquet file...\")\n",
        "    notes_for_processing = pd.read_parquet(cleaned_notes_parquet_path)\n",
        "    if 'analytical_df' in locals() and analytical_df.index.name == 'hadm_id' and 'hadm_id' in notes_for_processing.columns:\n",
        "        notes_for_processing['hadm_id'] = notes_for_processing['hadm_id'].astype(analytical_df.index.dtype)\n",
        "else:\n",
        "    # If not, load raw notes, clean them, and save the result\n",
        "    print(\"Cleaning notes and saving to parquet file...\")\n",
        "    tqdm.pandas() # Enable progress bar for pandas apply\n",
        "    temp_notes_df = dfs['discharge_notes'][['hadm_id', 'text']].copy()\n",
        "    temp_notes_df.dropna(subset=['text'], inplace=True)\n",
        "    if 'analytical_df' in locals():\n",
        "      temp_notes_df['hadm_id'] = temp_notes_df['hadm_id'].astype(analytical_df.index.dtype)\n",
        "\n",
        "    # Aggregate and clean notes\n",
        "    notes_for_processing = temp_notes_df.groupby('hadm_id')['text'].progress_apply(lambda x: ' '.join(x)).reset_index()\n",
        "    notes_for_processing['cleaned_text'] = notes_for_processing['text'].progress_apply(clean_clinical_text)\n",
        "\n",
        "    # Save the cleaned text to a parquet file for future use\n",
        "    notes_for_processing[['hadm_id', 'cleaned_text']].to_parquet(cleaned_notes_parquet_path, index=False)\n",
        "\n",
        "# Feature engineering with transformer embeddings (Bio_ClinicalBERT)\n",
        "# This approach replaces TF-IDF features with more semantically rich embeddings\n",
        "notes_with_cleaned_text = pd.read_parquet(cleaned_notes_parquet_path)\n",
        "if analytical_df.index.name == 'hadm_id' and 'hadm_id' in notes_with_cleaned_text.columns:\n",
        "    notes_with_cleaned_text['hadm_id'] = notes_with_cleaned_text['hadm_id'].astype(analytical_df.index.dtype)\n",
        "notes_with_cleaned_text.set_index('hadm_id', inplace=True)\n",
        "\n",
        "# Set up the model and tokenizer from Hugging Face\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_name = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name).to(device)\n",
        "model.eval() # Set model to evaluation mode\n",
        "\n",
        "# Define a function to get embeddings for a batch of texts\n",
        "def get_embeddings_batch(texts_batch, tokenizer, model, device, max_length=512):\n",
        "    # Tokenize the text batch\n",
        "    inputs = tokenizer(texts_batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
        "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
        "\n",
        "    # Get model outputs without calculating gradients\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        # Use the embedding of the [CLS] token as the representation for the entire text\n",
        "        cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
        "    return cls_embeddings\n",
        "\n",
        "# Align the cleaned notes with the main analytical dataframe to ensure correct order and matching\n",
        "aligned_notes_series = notes_with_cleaned_text['cleaned_text'].reindex(analytical_df.index).fillna('')\n",
        "\n",
        "# Generate embeddings in batches to manage memory\n",
        "all_texts = aligned_notes_series.tolist()\n",
        "all_embeddings = []\n",
        "batch_size = 32 # Adjust batch size based on GPU memory\n",
        "for i in tqdm(range(0, len(all_texts), batch_size), desc=\"Generating Embeddings\"):\n",
        "    batch_texts = all_texts[i:i + batch_size]\n",
        "    batch_embeddings = get_embeddings_batch(batch_texts, tokenizer, model, device)\n",
        "    all_embeddings.append(batch_embeddings)\n",
        "\n",
        "# Combine embeddings from all batches into a single numpy array\n",
        "final_embeddings_array = np.vstack(all_embeddings)\n",
        "\n",
        "# Create a dataframe from the embeddings\n",
        "embedding_dim = final_embeddings_array.shape[1]\n",
        "embedding_feature_names = [f\"emb_{j}\" for j in range(embedding_dim)]\n",
        "embeddings_df = pd.DataFrame(final_embeddings_array, columns=embedding_feature_names, index=analytical_df.index)\n",
        "\n",
        "# Merge the new transformer embedding features into the analytical dataframe\n",
        "analytical_df = analytical_df.merge(embeddings_df, on='hadm_id', how='left')\n",
        "analytical_df[embedding_feature_names] = analytical_df[embedding_feature_names].fillna(0)"
      ],
      "metadata": {
        "id": "xXPcFRlBmIa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Data"
      ],
      "metadata": {
        "id": "dJpoioY2mb8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the final analytical dataframe and the list of target labels to files\n",
        "analytical_df_save_path = os.path.join(project_base_path, \"analytical_df_with_transformer_embeddings.parquet\")\n",
        "analytical_df.to_parquet(analytical_df_save_path, index=True)\n",
        "target_labels_save_path = os.path.join(project_base_path, \"final_target_labels_composite.txt\")\n",
        "with open(target_labels_save_path, 'w') as f:\n",
        "    for label in final_target_labels_to_predict:\n",
        "        f.write(f\"{label}\\n\")\n",
        "print(\"Saved analytical dataframe with embeddings and target labels.\")"
      ],
      "metadata": {
        "id": "zMy6z65xmc7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading for Modeling"
      ],
      "metadata": {
        "id": "9A6CE09eml03"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the preprocessed data from the saved files"
      ],
      "metadata": {
        "id": "jcnRzIyIuf3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analytical_df_load_path = os.path.join(project_base_path, \"analytical_df_with_transformer_embeddings.parquet\")\n",
        "analytical_df = pd.read_parquet(analytical_df_load_path)\n",
        "target_labels_load_path = os.path.join(project_base_path, \"final_target_labels_composite.txt\")\n",
        "\n",
        "with open(target_labels_load_path, 'r') as f:\n",
        "    final_target_labels_to_predict = [line.strip() for line in f]\n",
        "\n",
        "print(\"Loaded preprocessed data and target labels.\")"
      ],
      "metadata": {
        "id": "CxmWotg9mmOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Separate features (X) and targets (Y)"
      ],
      "metadata": {
        "id": "RnFfSTeVmtcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valid_target_labels = [label for label in final_target_labels_to_predict if label in analytical_df.columns]\n",
        "Y = analytical_df[valid_target_labels]\n",
        "X = analytical_df.drop(columns=valid_target_labels)"
      ],
      "metadata": {
        "id": "xGY4AzVgmuhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drop datetime columns from the feature set as they are not directly used in the model\n"
      ],
      "metadata": {
        "id": "_QoAfe5vmyjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datetime_cols_to_drop = ['admittime', 'dischtime']\n",
        "actual_cols_to_drop_from_X = [col for col in datetime_cols_to_drop if col in X.columns]\n",
        "\n",
        "if actual_cols_to_drop_from_X:\n",
        "    X = X.drop(columns=actual_cols_to_drop_from_X)"
      ],
      "metadata": {
        "id": "i_l16dhmmzez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train/Validation/Test Split"
      ],
      "metadata": {
        "id": "d7DC8Koym3kH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare for group-based data splitting to prevent data leakage Patients can have multiple admissions, so we need to ensure all admissions for a single patient are in the same split (train, validation, or test)"
      ],
      "metadata": {
        "id": "qe29VC4-nDrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "admissions_df = dfs.get('admissions', pd.read_csv(os.path.join(project_base_path, \"Data/hosp/admissions.csv.gz\")))\n",
        "subject_id_map_df = admissions_df[['hadm_id', 'subject_id']].copy()\n",
        "subject_id_map_df.dropna(subset=['hadm_id', 'subject_id'], inplace=True)\n",
        "subject_id_map_df['hadm_id'] = subject_id_map_df['hadm_id'].astype(X.index.dtype)\n",
        "\n",
        "# Create a series that maps each admission (hadm_id) to its patient (subject_id)\n",
        "groups_series = X.index.map(subject_id_map_df.set_index('hadm_id')['subject_id'])\n",
        "groups_for_split_array = np.array(groups_series)"
      ],
      "metadata": {
        "id": "vmhWHaRTm4n1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split data into training/validation (80%) and test (20%) sets, keeping patient data together\n"
      ],
      "metadata": {
        "id": "hYwC41GhnIML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gss_tv_test = GroupShuffleSplit(n_splits=1, test_size=0.20, random_state=42)\n",
        "train_val_idx, test_idx = next(gss_tv_test.split(X, Y, groups_for_split_array))\n",
        "X_train_val, X_test = X.iloc[train_val_idx], X.iloc[test_idx]\n",
        "Y_train_val, Y_test = Y.iloc[train_val_idx], Y.iloc[test_idx]\n",
        "groups_of_train_val_set = groups_for_split_array[train_val_idx]"
      ],
      "metadata": {
        "id": "EzgmqkxCm-c5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "plit the training/validation set further into training (75% of this set) and validation (25%)\n"
      ],
      "metadata": {
        "id": "z2qRxdb6nJxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gss_train_val = GroupShuffleSplit(n_splits=1, test_size=0.25, random_state=42) # 0.25 * 0.8 = 0.2\n",
        "train_idx, val_idx = next(gss_train_val.split(X_train_val, Y_train_val, groups_of_train_val_set))\n",
        "X_train, X_val = X_train_val.iloc[train_idx], X_train_val.iloc[val_idx]\n",
        "Y_train, Y_val = Y_train_val.iloc[train_idx], Y_train_val.iloc[val_idx]"
      ],
      "metadata": {
        "id": "HReHCr8InBtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Validation set shape: {X_val.shape}\")\n",
        "print(f\"Test set shape: {X_test.shape}\")"
      ],
      "metadata": {
        "id": "itYshpNsnM0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Development"
      ],
      "metadata": {
        "id": "V7kXyknPuavL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Definition"
      ],
      "metadata": {
        "id": "_SktaX6mnSBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the neural network architecture"
      ],
      "metadata": {
        "id": "XatNDSXfuj3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_features = X_train.shape[1]\n",
        "output_labels = Y_train.shape[1]\n",
        "model = keras.Sequential([\n",
        "    layers.Input(shape=(input_features,), name=\"input_layer\"),\n",
        "    layers.Dense(512, activation=\"relu\", name=\"dense_1\"),\n",
        "    layers.Dropout(0.4, name=\"dropout_1\"),\n",
        "    layers.Dense(256, activation=\"relu\", name=\"dense_2\"),\n",
        "    layers.Dropout(0.3, name=\"dropout_2\"),\n",
        "    layers.Dense(output_labels, activation=\"sigmoid\", name=\"output_layer\") # Sigmoid for multi-label classification\n",
        "], name=\"drug_recommender_model\")\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "ttRyDU0ZnP0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Compilation and Training"
      ],
      "metadata": {
        "id": "CAw3q2MonZch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the metrics to monitor during training and evaluation\n",
        "METRICS = [\n",
        "    keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "    keras.metrics.Precision(name='precision'),\n",
        "    keras.metrics.Recall(name='recall'),\n",
        "    keras.metrics.AUC(name='auc', multi_label=True, num_labels=output_labels),\n",
        "]"
      ],
      "metadata": {
        "id": "a3aCKtshnbXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model with an optimizer, loss function, and metrics\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.0005),\n",
        "    loss=\"binary_crossentropy\", # Appropriate for multi-label classification\n",
        "    metrics=METRICS\n",
        ")\n",
        "\n",
        "# Set up callbacks to improve the training process\n",
        "model_checkpoint_path = os.path.join(project_base_path, \"best_drug_model.keras\")\n",
        "callbacks = [\n",
        "    # Save the best version of the model based on validation AUC\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=model_checkpoint_path,\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_auc\",\n",
        "        mode=\"max\",\n",
        "        verbose=1\n",
        "    ),\n",
        "    # Stop training early if the validation AUC doesn't improve for a number of epochs\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_auc\",\n",
        "        patience=10,\n",
        "        mode=\"max\",\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    )\n",
        "]"
      ],
      "metadata": {
        "id": "M08-I0pYnd-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define training parameters\n"
      ],
      "metadata": {
        "id": "N5mPlkMynjIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 50\n",
        "BATCH_SIZE = 128"
      ],
      "metadata": {
        "id": "bg2XWmihnh60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train,\n",
        "    Y_train,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=(X_val, Y_val),\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "xC0hxX89nlwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Results & Evaluation"
      ],
      "metadata": {
        "id": "63D46jWdnpFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the final model on the held-out test set\n",
        "print(\"\\nEvaluating model on the test set...\")\n",
        "test_loss, test_accuracy, test_precision, test_recall, test_auc = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Test Precision: {test_precision:.4f}\")\n",
        "print(f\"Test Recall: {test_recall:.4f}\")\n",
        "print(f\"Test AUC: {test_auc:.4f}\")"
      ],
      "metadata": {
        "id": "X5JvHdPAnqk-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}